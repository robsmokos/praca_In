import os
import traci
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# ğŸ”¹ StaÅ‚e konfiguracyjne
SUMO_BINARY = "sumo-gui"
CONFIG_FILE = "c:/DATA/ROB/PRACA/praca_In/_KOD/2x2_END_swiatlaAI/2x2.sumocfg"
TLS_IDS = ["P4", "P5", "P8", "P9"]
NUM_TLS = len(TLS_IDS)
NUM_PHASES = 4

# ğŸ”¹ Model Actor-Critic
class ActorCritic(tf.keras.Model):
    def __init__(self, num_tls, num_phases):
        super().__init__()
        self.common = tf.keras.Sequential([
            layers.Dense(256, activation="relu", kernel_initializer="he_normal"),
            layers.Dense(128, activation="relu", kernel_initializer="he_normal"),
            layers.Dense(64, activation="relu", kernel_initializer="he_normal")
        ])
        self.actor = layers.Dense(num_tls * num_phases, activation="softmax", name="actor")
        self.critic = layers.Dense(1, name="critic")

    def call(self, state):
        x = self.common(state)
        actor_output = self.actor(x)
        critic_output = self.critic(x)
        return actor_output, critic_output

# ğŸ”¹ Pobieranie stanu skrzyÅ¼owaÅ„
def get_state():
    max_queue_length = 400
    max_waiting_time = 10000  

    queue_lengths = np.array([
        sum(traci.lane.getLastStepHaltingNumber(lane) for lane in traci.trafficlight.getControlledLanes(tls_id))
        for tls_id in TLS_IDS
    ], dtype=np.float32) / max_queue_length

    waiting_times = np.array([
        sum(traci.lane.getWaitingTime(lane) for lane in traci.trafficlight.getControlledLanes(tls_id))
        for tls_id in TLS_IDS
    ], dtype=np.float32) / max_waiting_time

    return np.concatenate([queue_lengths, waiting_times]).reshape(1, -1)

# ğŸ”¹ WybÃ³r akcji
def choose_action(action_probs, num_tls, num_phases):
    action_probs = action_probs.numpy().reshape(num_tls, num_phases)
    action_probs = np.clip(action_probs, 0, None)

    for i in range(num_tls):
        row_sum = np.sum(action_probs[i])
        if np.isclose(row_sum, 0.0):
            action_probs[i] = np.ones(num_phases) / num_phases
        else:
            action_probs[i] /= row_sum

    actions = [np.random.choice(num_phases, p=probs) for probs in action_probs]
    return actions

# ğŸ”¹ Ustawianie faz Å›wiateÅ‚
def apply_action(actions):
    phases = ["GGgrrrGGgrrr", "yyyyyyyyyyyy", "rrrGGgrrrGGg", "GGGGGGGGGGGG"]
    for tls_id, action in zip(TLS_IDS, actions):
        traci.trafficlight.setRedYellowGreenState(tls_id, phases[action])

# ğŸ”¹ Testowanie modelu
def test_model(model_path, steps=500):
    print(f"ğŸ” Testowanie modelu z wagami: {model_path}")

    model = ActorCritic(NUM_TLS, NUM_PHASES)

    # ğŸ”¹ WAÅ»NE: Najpierw wykonujemy forward pass na sztucznym wejÅ›ciu, aby poprawnie zbudowaÄ‡ model
    dummy_input = np.zeros((1, NUM_TLS * 2), dtype=np.float32)  # Dopasowujemy wejÅ›cie do ksztaÅ‚tu modelu
    _ = model(dummy_input)  # Wykonujemy wywoÅ‚anie, aby zainicjalizowaÄ‡ warstwy

    model.load_weights(model_path)
    print("âœ… Wagi modelu zaÅ‚adowane!")

    try:
        traci.start([SUMO_BINARY, "-c", CONFIG_FILE, "--start"])
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d uruchamiania SUMO: {e}")
        return

    total_reward = 0
    rewards_per_step = []

    for step in range(steps):
        state = get_state()
        action_probs, _ = model(state)
        actions = choose_action(action_probs, NUM_TLS, NUM_PHASES)
        apply_action(actions)
        traci.simulationStep()

        reward = -sum(
            sum(traci.lane.getLastStepHaltingNumber(lane) for lane in traci.trafficlight.getControlledLanes(tls_id))
            for tls_id in TLS_IDS
        )
        
        total_reward += reward
        rewards_per_step.append(total_reward)

        print(f"ğŸ”„ Step {step}: Wybrane akcje -> {actions}, Nagroda: {reward:.2f}")

    print(f"âœ… Test zakoÅ„czony! CaÅ‚kowita nagroda: {total_reward}")
    traci.close()

    # ğŸ”¹ Rysowanie wykresu nagrÃ³d w czasie
    plt.figure(figsize=(10, 5))
    plt.plot(range(steps), rewards_per_step, label="Nagroda w czasie", color='b')
    plt.xlabel("Krok symulacji")
    plt.ylabel("Nagroda")
    plt.title(f"Nagroda dla modelu {model_path}")
    plt.legend()
    plt.grid(True)
    plt.show()

    return total_reward

# ğŸ”¹ Uruchomienie testowania
if __name__ == "__main__":
    test_model("epizod_63.weights.h5", steps=6000)  # ğŸ”¹ ZmieÅ„ Å›cieÅ¼kÄ™ na poprawnÄ…
